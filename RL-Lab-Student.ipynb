{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement-Learning Reference\n",
    "Reinforcement Assignment Practical\n",
    "\n",
    "### Source:\n",
    "These are the files required to build your reinforcement learning algorithm. \n",
    "\n",
    "- [common.py](common.py) with constants\n",
    "- [util.py](util.py) with util functions\n",
    "- [game.py](game.py) with drawing calls\n",
    "- [environment.py](environment.py) contains the scenario behavior\n",
    "- [agent.py](agent.py) contains training components, such as environment interaction and previous state\n",
    "\n",
    "### Assignment\n",
    "The goal of this assignment is to implement the core of the Q-Learning algorithm. You will be responsible for implementing three distinct methods:\n",
    "- The exploration function(**f()** method)\n",
    "- The Q-Learning update method (**get_action()** method)\n",
    "- Implement a decreasing function for the learning rate (**alpha()** method)\n",
    "\n",
    "In this scenario we help the agent, via reinforcement learning, to navigate and maximise rewards within a map, aiming to reach the move between an initial state and the goal state, represented by a treasure chest. In some scenarios there will be a rupee that the agent can gather. The rewards are +50 for reaching the chest, +40 for getting the rupee and -1 for any other tile.\n",
    "\n",
    "This assignment is not graded. Thus no tests are provided.\n",
    "\n",
    "### Execution\n",
    "The execution of this assignment can be done entirely in this Jupyter Notebook, or in two distinct python files. If you want to program/test outside of Jupyter just follow these instructions. Please note that currently there is a problem with pygame (a package responsible for displaying the agent moving in the environment) and Jupyter, which causes the jupyter kernel to crash after closing the pygame window. Instead of closing the window, press **space bar** to close the pygame window. To install pygame, run:\n",
    "\n",
    "```\n",
    "pip install pygame --user\n",
    "```\n",
    "\n",
    "To be able to visualize plots, install matplotlib by running:\n",
    "\n",
    "```\n",
    "pip install matplotlib --user\n",
    "```\n",
    "\n",
    "In order to test your code and get the convergence episode, you can use the environment.py file:\n",
    "```\n",
    "python environment.py [Map]\n",
    "```\n",
    "\n",
    "To check the converged solution of your algorithm, you can run the GUI to see the agent executing the learned policy in each map.\n",
    "```\n",
    "python game.py [Map]\n",
    "```\n",
    "\n",
    "### Implementation\n",
    "\n",
    "In this file, we provide the basic architecture to build your Q-Learning algorithm. Additionally, you can implement your algorithm in the [link_ref.py](link_ref.py) file, if you wish to work outside of Jupyter Notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration function\n",
    "Implement the optimistic estimate function described by the following equation.\n",
    "$$\n",
    "f(u,n) = \\begin{cases}\n",
    "\t\t\t\t\tR^{+} & \\mathit{if} \\text{ }n < N_{e} \\\\\n",
    "\t\t\t\t\tu & \\mathit{otherwise}\n",
    "\t\t\t\t   \\end{cases}\n",
    "$$\n",
    "The parameter **self** here is an instance of the Link class. Consider that the $N_{e}$ value is the `self.exploration` attribute and $R^+$ is the `self.r_plus` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receives a q-value and returns a utility\n",
    "def f(self, value, freq):\n",
    "    from random import randint\n",
    "    return randint(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "Implement the Q-Learning update equation. Consider the learning rate as a fixed value for now. Again, **self** refers to an instance of the Link class. \n",
    "You can check the reward of a state with:\n",
    "```\n",
    "reward_prime = self.env.state_reward((state.x, state.y))\n",
    "```\n",
    "To check if a state is terminal, we use this:\n",
    "```\n",
    "if self.env.terminal((state.x, state.y)):\n",
    "```\n",
    "Finally, when creating a entry on the **q-table** with the **None** action we use:\n",
    "```\n",
    " self.q_values[qvalue.QValue(state, NO_OP)] = reward_prime\n",
    "```\n",
    "<img src=\"https://user-images.githubusercontent.com/4201145/45648565-4d0c0580-ba9f-11e8-82fd-1a4f127c1959.png\" width=\"70%\" height=\"70%\"/>\n",
    "\n",
    "The ***argmax*** and ***max*** functions are already implemented as ***argmax_a(s)*** and ***max_a(s)***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(self, state):\n",
    "    \"\"\"This function corresponds to the q-learning-agent(percept) \n",
    "    function in Russel and Norvig's book\n",
    "    \"\"\"\n",
    "    if self.env.terminal((state.x, state.y)):\n",
    "        if QValue(state, 'NO_OP') not in self.frequency:\n",
    "            self.frequency[QValue(state, 'NO_OP')] = 0\n",
    "        self.frequency[QValue(state, 'NO_OP')] += 1\n",
    "        self.q_values[QValue(state, 'NO_OP')] = self.env.state_reward((state.x, state.y))\n",
    "    if self.p_state is not None:\n",
    "        qv = QValue(self.p_state,self.p_action)\n",
    "        if qv not in self.frequency:\n",
    "            self.frequency[qv] = 0\n",
    "        self.frequency[qv] += 1\n",
    "        q_sa = self.q_value_check(qv)\n",
    "        ### YOUR CODE HERE\n",
    "        #Implement Q-Learning update here\n",
    "        #self.q_values[qv] = update rule here\n",
    "        ### END CODE\n",
    "    self.p_state = state\n",
    "    self.p_action = self.argmax_a(state)\n",
    "    self.p_reward = self.env.state_reward((state.x, state.y))\n",
    "    return self.p_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate\n",
    "Choose a decreasing function based on how many times a state has been visited and implement it here. Again, **self** refers to an instance of the Link class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(self,qv):\n",
    "    ### YOUR CODE HERE\n",
    "    # Implement here a more sophisticated learning rate\n",
    "    return 0.9\n",
    "    ### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base code\n",
    "Base code of the Link class. You don't need to implement anything here. The attributes of this class are:\n",
    "- self.q_values -> A dictionary that maps a Q-Value to a utility.\n",
    "- self.frequency -> A dictionary that maps a Q-Value to the number of times it has been visited.\n",
    "- self.state -> The actual state (used for internal methods, don't need to use this on the Q-Learning implementation)\n",
    "- self.reward -> The actual reward (used for internal methods, don't need to use this on the Q-Learning implementation)\n",
    "- self.action -> The actual action (used for internal methods, don't need to use this on the Q-Learning implementation)\n",
    "- self.p_state -> The previous state.\n",
    "- self.p_reward -> The previous reward.\n",
    "- self.p_action -> The previous action.\n",
    "- self.gamma -> Discount factor.\n",
    "- self.r_plus -> The highest reward in the environment.\n",
    "- self.exploration -> The exploration threshold.\n",
    "- self.env -> A instance of the environment (internal use).\n",
    "- self.prev_qtable -> Previous Q-Table, used for checking the convergence (internal use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Four spaces as indentation [no tabs]\n",
    "# Standard Q-Learning implementation.\n",
    "import math, copy, random, logging\n",
    "import numpy as np\n",
    "from qvalue import *\n",
    "from common import *\n",
    "from util import *\n",
    "from agent import *\n",
    "\n",
    "class Link(Agent):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        Agent.__init__(self)\n",
    "        self.q_values = dict()\n",
    "        self.frequency = dict()\n",
    "        self.state = None\n",
    "        self.reward = None\n",
    "        self.action = NO_OP\n",
    "        self.p_state = None \n",
    "        self.p_reward = None\n",
    "        self.p_action = None\n",
    "        self.gamma = 0.9\n",
    "        self.r_plus = 50\n",
    "        self.exploration = 1\n",
    "        self.env = None\n",
    "        self.prev_qtable = dict()\n",
    "\n",
    "    def reset(self, env):\n",
    "        \"\"\"\n",
    "        Reset the state to the initial environment state\n",
    "        \"\"\"\n",
    "        self.state = env.init\n",
    "\n",
    "    def train(self, env):\n",
    "        \"\"\"\n",
    "        Execute MAX_TRAINING_EPISODES rounds or until converge.\n",
    "        \"\"\"\n",
    "        print('It will converge at', CONVERGENCE_THRESHOLD)\n",
    "\n",
    "        self.reset(env)\n",
    "        self.env = env\n",
    "\n",
    "        executions = 0\n",
    "        last_plan = []\n",
    "        while executions < MAX_TRAINING_EPISODES:\n",
    "            self.state = self.make_state(env)\n",
    "            action = self.get_action(self.state)\n",
    "            last_plan.append(action)\n",
    "            self.env.execute(action)\n",
    "            if env.terminal((self.state.x, self.state.y)):\n",
    "                executions += 1\n",
    "                \n",
    "                self.p_state = self.p_action = self.p_reward = self.state = self.action = self.reward = None\n",
    "                self.reset(env)\n",
    "\n",
    "                if self.converged():\n",
    "                    print('Converged!')\n",
    "                    break\n",
    "                else:\n",
    "                    last_plan = []\n",
    "                    self.prev_qtable = copy.deepcopy(self.q_values)\n",
    "\n",
    "                #print('Episode', executions, ': convergence %', self.convergence)\n",
    "        \n",
    "        print('Episode', executions, '- Convergence metric is', self.convergence)\n",
    "        if (executions == MAX_TRAINING_EPISODES): print('Max training episodes reached!')\n",
    "        print('Last plan executed: ', [ACTIONS_NAMES[x] for x in last_plan])\n",
    "\n",
    "    def alpha(self, qv):\n",
    "        \"\"\"\n",
    "        Alpha value, currently returning 0.9 because it converges pretty fast. \n",
    "        \"\"\"\n",
    "        return alpha(self,qv)\n",
    "\n",
    "    def f(self, value, freq):\n",
    "        \"\"\"\n",
    "        Exploration function. Use maxreward if the q_value was not explored.\n",
    "        \"\"\"\n",
    "        return f(self, value, freq)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return get_action(self,state)   \n",
    "\n",
    "    def q_value_check(self, qv):\n",
    "        if qv in self.q_values:\n",
    "            return self.q_values[qv]\n",
    "        return 0.0\n",
    "    \n",
    "    def freq_check(self, qv):\n",
    "        if qv in self.frequency:\n",
    "            return self.frequency[qv]\n",
    "        return 0\n",
    "\n",
    "    def max_a(self, state):\n",
    "        return np.max([self.q_value_check((QValue (state,action))) for action in self.env.available_actions((state.x, state.y))])\n",
    "    \n",
    "    def argmax_a(self, state):\n",
    "        action_index = np.argmax([self.f(self.q_value_check((QValue (state,action))), self.freq_check((QValue (state,action)))) for action in self.env.available_actions((state.x, state.y))])\n",
    "        action = self.env.available_actions((state.x, state.y))[action_index]\n",
    "        return action\n",
    "\n",
    "    def make_state(self, env):\n",
    "        \"\"\"\n",
    "        Build state using position and rupees.\n",
    "        \"\"\"\n",
    "        return State(env.state[0], env.state[1], env.rupees)\n",
    "\n",
    "    def return_qvalue(self, qvalue):\n",
    "        if qvalue in self.q_values:\n",
    "            return self.q_values[qvalue]\n",
    "        return 0\n",
    "\n",
    "    def converged(self):\n",
    "        \"\"\"\n",
    "        Return True if the change between previous util table and current util table\n",
    "        are smaller than the convergence_threshold.\n",
    "        \"\"\"\n",
    "        self.convergence = self.convergence_metric()\n",
    "        self.metric_history.append(self.convergence)\n",
    "        return self.convergence < CONVERGENCE_THRESHOLD\n",
    "\n",
    "    def run(self, env):\n",
    "        \"\"\"\n",
    "        Execute actions.\n",
    "        \"\"\"\n",
    "        self.action = self.argmax_a(self.make_state(env))\n",
    "        #print \"Running action: \", ACTIONS_NAMES[self.action]\n",
    "        self.state, self.reward = env.execute(self.action)\n",
    "        return self.action, self.state\n",
    "\n",
    "\n",
    "    def convergence_metric(self):\n",
    "        \"\"\"\n",
    "        Return the convergence metric.\n",
    "        \"\"\"\n",
    "        prev_qvalues = np.array([0.0 if qv not in self.prev_qtable.keys() else self.prev_qtable[qv] for qv in self.q_values.keys()])\n",
    "        curr_qvalues = np.array([v for v in self.q_values.values()])\n",
    "        \n",
    "        return numpy.linalg.norm(prev_qvalues - curr_qvalues)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Agent\n",
    "In this cell, we train the agent. You can change the map and add any code you like here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from game import *\n",
    "    pg = True\n",
    "except ImportError:\n",
    "    pg = False\n",
    "    \n",
    "from environment import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "sx, sy, map_data, map_width, map_height = read_map(\"maps/medium.txt\")\n",
    "\n",
    "agt = Link()\n",
    "\n",
    "env = Environment(sx, sy, map_data, map_width, map_height)\n",
    "\n",
    "start_time = time.time()\n",
    "agt.train(env)\n",
    "elapsed_time = time.time() - start_time\n",
    "print('It took', elapsed_time,'seconds to train.' )\n",
    "\n",
    "try:\n",
    "    from matplotlib import pyplot as plt\n",
    "    %matplotlib inline\n",
    "    plt.plot(agt.metric_history)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Convergence metric\")\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"Could not import plot library, try installing with 'pip install matplotlib --user'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pg:\n",
    "    #Comment this line if you do not want to use the UI\n",
    "    Game(env, agt)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
